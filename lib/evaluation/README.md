# Evaluation System - LLM as Judge

SystÃ¨me d'Ã©valuation personnalisÃ© utilisant Gemini comme juge LLM pour Ã©valuer la qualitÃ© des rÃ©ponses gÃ©nÃ©rÃ©es.

## ğŸ¯ Metrics Ã‰valuÃ©es

### 1. **Correctness** (0-1)
Ã‰value si la gÃ©nÃ©ration inclut tous les faits clÃ©s de la vÃ©ritÃ© de terrain et si chaque fait est factuellement supportÃ©.

### 2. **Context Precision** (0-1)
VÃ©rifie si le contexte fourni Ã©tait utile pour arriver Ã  la rÃ©ponse donnÃ©e.

### 3. **Answer Relevance** (0-1)
GÃ©nÃ¨re une question pour la rÃ©ponse donnÃ©e et identifie si la rÃ©ponse est Ã©vasive ou engagÃ©e.

### 4. **Faithfulness** (0-1)
Analyse la complexitÃ© de chaque phrase et dÃ©compose la rÃ©ponse en dÃ©clarations vÃ©rifiables.

## ğŸš€ Utilisation

### Via l'interface Web

1. DÃ©marrer le serveur de dÃ©veloppement :
```bash
pnpm dev
```

2. AccÃ©der au dashboard :
```
http://localhost:3000/evaluation
```

3. Utiliser l'interface pour :
   - Lancer de nouvelles Ã©valuations
   - Visualiser les rÃ©sultats avec graphiques
   - Consulter l'historique
   - Exporter en JSON/CSV

### Via le CLI

```bash
# Ã‰valuation basique
pnpm eval

# Avec options
pnpm eval --dataset esn_qa_dataset --max-samples 10

# SpÃ©cifier le modÃ¨le
pnpm eval --model gemini-1.5-pro

# SpÃ©cifier l'output
pnpm eval --output ./my-results.json
```

## ğŸ“ Structure

```
lib/evaluation/
â”œâ”€â”€ types.ts                  # Types TypeScript
â”œâ”€â”€ evaluator.ts              # Orchestrateur principal
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ templates.ts          # Templates de prompts
â”œâ”€â”€ judges/
â”‚   â””â”€â”€ gemini-judge.ts       # Gemini LLM as Judge
â””â”€â”€ metrics/
    â”œâ”€â”€ correctness.ts
    â”œâ”€â”€ context-precision.ts
    â”œâ”€â”€ answer-relevance.ts
    â””â”€â”€ faithfulness.ts

components/evaluation/
â”œâ”€â”€ dashboard.tsx             # Dashboard principal
â”œâ”€â”€ metrics-chart.tsx         # Graphiques
â”œâ”€â”€ results-table.tsx         # Table dÃ©taillÃ©e
â””â”€â”€ evaluation-runner.tsx     # Interface de lancement

app/
â”œâ”€â”€ (evaluation)/evaluation/  # Page Next.js
â””â”€â”€ api/evaluation/
    â”œâ”€â”€ run/route.ts          # POST - Lancer Ã©valuation
    â”œâ”€â”€ results/route.ts      # GET - RÃ©cupÃ©rer rÃ©sultats
    â””â”€â”€ export/route.ts       # GET - Exporter
```

## âš™ï¸ Configuration

### Variables d'environnement

Ajouter dans `.env.local` :

```bash
# Gemini API Key (requis)
GOOGLE_GENERATIVE_AI_API_KEY=your_api_key_here
```

### Format du Dataset CSV

Le dataset doit contenir les colonnes suivantes :

```csv
id,query,generation,ground_truth,context,answer
sample_1,"Question?","Generated answer","Expected answer","Context text","Answer text"
```

**Colonnes requises :**
- `id` - Identifiant unique
- `query` - Question posÃ©e
- `generation` - RÃ©ponse gÃ©nÃ©rÃ©e
- `ground_truth` - RÃ©ponse attendue

**Colonnes optionnelles :**
- `context` - Contexte fourni (pour Context Precision)
- `answer` - RÃ©ponse alternative (pour Answer Relevance)

## ğŸ“Š Exemples de RÃ©sultats

### Dashboard Web
![Dashboard](docs/evaluation-dashboard.png)

### Export JSON
```json
{
  "evaluation_id": "abc123",
  "dataset_name": "esn_qa_dataset",
  "total_samples": 100,
  "metrics": {
    "correctness": {
      "average": 0.78,
      "min": 0.1,
      "max": 1.0,
      "distribution": {
        "0.8-1.0": 45,
        "0.6-0.8": 30,
        ...
      }
    }
  }
}
```

### Export CSV
```csv
sample_id,query,generation,correctness_score,correctness_reasoning,...
sample_1,"Question?","Answer",0.85,"The generation correctly...",...
```

## ğŸ”§ API Endpoints

### POST /api/evaluation/run
Lancer une nouvelle Ã©valuation

```typescript
{
  dataset_name: 'esn_qa_dataset',
  model_name: 'gemini-2.5-flash-lite',
  max_samples: 10,
  metrics: ['correctness', 'faithfulness'],
  save_results: true
}
```

### GET /api/evaluation/results
Lister toutes les Ã©valuations

### GET /api/evaluation/results?id=xxx
RÃ©cupÃ©rer une Ã©valuation spÃ©cifique

### GET /api/evaluation/export?id=xxx&format=csv
Exporter les rÃ©sultats

## ğŸ’¡ Tips

### Optimiser les coÃ»ts
- Utiliser `gemini-2.5-flash-lite` (gratuit/trÃ¨s Ã©conomique)
- Limiter avec `--max-samples` pour les tests
- SÃ©lectionner uniquement les metrics nÃ©cessaires

### AmÃ©liorer la prÃ©cision
- Utiliser `gemini-1.5-pro` pour plus de prÃ©cision
- Fournir des ground truth dÃ©taillÃ©es
- Inclure du contexte pertinent

### Performance
- Le systÃ¨me Ã©value ~2-3 samples/seconde
- Utilise retry automatique en cas d'erreur
- GÃ¨re le rate limiting de Gemini

## ğŸ“ˆ Roadmap

- [ ] Support de modÃ¨les Judge supplÃ©mentaires (Claude, GPT-4)
- [ ] Metrics custom dÃ©finissables via UI
- [ ] Comparaison multi-modÃ¨les
- [ ] Streaming des rÃ©sultats en temps rÃ©el
- [ ] IntÃ©gration Langfuse pour tracing

## ğŸ› Troubleshooting

### "GOOGLE_GENERATIVE_AI_API_KEY is required"
Ajouter la clÃ© API dans `.env.local`

### "Failed to parse CSV"
VÃ©rifier que le CSV a les colonnes requises

### Rate limit errors
Utiliser `--max-samples` pour limiter ou attendre quelques minutes

## ğŸ“ License

MIT
